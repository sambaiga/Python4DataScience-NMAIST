{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from math import sqrt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3)\n",
    "fig_width = 6.9\n",
    "golden_mean = (sqrt(5)-1.0)/2.0    # Aesthetic ratio\n",
    "fig_height = fig_width*golden_mean # height in inches\n",
    "\n",
    "params = {\n",
    "   'axes.labelsize': 8,\n",
    "   'text.latex.preamble': ['\\\\usepackage{gensymb}'],\n",
    "   'font.size': 10,\n",
    "    'axes.labelsize': 10, # fontsize for x and y labels (was 10)\n",
    "    'axes.titlesize': 12,\n",
    "   'legend.fontsize': 8,\n",
    "   'xtick.labelsize': 10,\n",
    "   'ytick.labelsize': 10,\n",
    "   'text.usetex': True,\n",
    "   'figure.figsize': [fig_width,fig_height],\n",
    "    'font.family': 'serif'\n",
    "   }\n",
    "rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We aims to give an accessible introduction to how to use machine learning techniques using [scikit-learn](http://scikit-learn.org/) for your own projects and datasets. Among the ML libraries available today, scikit-learn shines as one of the best options.\n",
    "\n",
    "[Scikit-learn](http://scikit-learn.org/) is a Python open source library designed to tackle Machine Learning problems from beginning to end. It is used and well praised by big companies like Evernote, Spotify etc as shown [here](http://scikit-learn.org/stable/testimonials/testimonials.html)\n",
    "\n",
    "\n",
    "In this post I will show you step by step how to create a machine learning experiment with  Scikit-learn that allows you to predict whether you or your friends would have survived the sinking of the titanic!.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn's Estimator API\n",
    "\n",
    "Every machine learning algorithm in Scikit-Learn is implemented via the Estimator API, which provides a consistent interface for a wide range of machine learning applications.\n",
    "\n",
    "The steps in using the Scikit-Learn estimator API are as follows:\n",
    "\n",
    "1. Choose a class of model by importing the appropriate estimator class from Scikit-Learn.\n",
    "2. Choose model hyperparameters by instantiating this class with desired values.\n",
    "   * Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments the estimator classes.\n",
    "3. Arrange data into a features matrix and target vector following the discussion above.\n",
    "4. Fit the model to your data by calling the **fit()** method of the model instance.\n",
    "5. Apply the Model to new data:\n",
    "  * For supervised learning, often we predict labels for unknown data using the **predict()** method.\n",
    "  * For unsupervised learning, we often transform or infer properties of the data using the **transform()** or **predict()** method.\n",
    "  \n",
    "Let us apply this step in the following example  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Representation in Scikit-Learn\n",
    "\n",
    "### Features matrix\n",
    "\n",
    "A two-dimensional numerical array or matrix with shape [$n_{samples}$, $n_{features}$]. By convention, this features matrix is often stored in a variable named $X$.\n",
    "\n",
    "The samples (i.e., rows) always refer to the individual objects described by the dataset\n",
    "\n",
    "The features (i.e., columns) always refer to the distinct observations that describe each sample.\n",
    "\n",
    "\n",
    "### Target array\n",
    "\n",
    "One dimensional, with length $n_{samples}$ usually the quantity we want to predict from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data =pd.read_csv('Data/Titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn will expect numeric values and no blanks, so first we need to do a bit more wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'Sex' is stored as a text value. We should convert (or 'map') it into numeric binaries \n",
    "# so it will be ready for scikit-learn.\n",
    "data['Sex'] = data['Sex'].map({'male': 0,'female': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's also drop the 'Cabin','Embarked' 'Ticket' and 'Name' columns\n",
    "data = data.drop(['Cabin'], axis=1)\n",
    "data = data.drop(['Embarked'], axis=1)\n",
    "data = data.drop(['Name'], axis=1)\n",
    "data = data.drop(['Ticket'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Handle missing data\n",
    "data['Age'].fillna(data['Age'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  Sex   Age  SibSp  Parch     Fare\n",
       "0            1         0       3    0  22.0      1      0   7.2500\n",
       "1            2         1       1    1  38.0      1      0  71.2833\n",
       "2            3         1       3    1  26.0      0      0   7.9250\n",
       "3            4         1       1    1  35.0      1      0  53.1000\n",
       "4            5         0       3    0  35.0      0      0   8.0500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrange data into a features matrix and target vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "X = data[features]\n",
    "y = data.Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluate Some Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to create some models of the data and estimate their accuracy on unseen data.\n",
    "\n",
    "Here is what we are going to cover in this step:\n",
    "\n",
    "* Separate out a validation dataset.\n",
    "* Set-up the test harness to use 10-fold cross validation.\n",
    "* Build 5 different models to predict species from flower measurements\n",
    "* Select the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Validation Dataset\n",
    "\n",
    "A better sense of a model's performance can be found using what's known as a holdout set: that is, we hold back some subset of the data from the training of the model, and then use this holdout set to check the model performance. This splitting can be done using the train_test_split utility in Scikit-Learn. We will split the loaded dataset into two, 80% of which we will use to train our models and 20% that we will hold back as a validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split the data with 20% in each set\n",
    "validation_size = 0.20\n",
    "seed = 7\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the metric of ‘accuracy‘ to evaluate models. This is a ratio of the number of correctly predicted instances in divided by the total number of instances in the dataset multiplied by 100 to give a percentage (e.g. 95% accurate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Models\n",
    "\n",
    "We don’t know which algorithms would be good on this problem or what configurations to use. We get an idea from the plots that some of the classes are partially linearly separable in some dimensions, so we are expecting generally good results.\n",
    "\n",
    "Let’s evaluate 4 different algorithms:\n",
    "\n",
    "* Logistic Regression (LR)\n",
    "* K-Nearest Neighbors (KNN).\n",
    "* Random Forest\n",
    "* Gaussian Naive Bayes (NB).\n",
    "* Support Vector Machines (SVM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION¶\n",
    "A logistic regression mathematically calculates the decision boundary between the possibilities. It looks for a straight line that represents a cutoff that most accurately represents the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining the model with its associated parameters\n",
    "from sklearn.linear_model import LogisticRegression   # 1. choose model class\n",
    "model = LogisticRegression()               # 2. instantiate model\n",
    "model.fit(X_train, y_train) \n",
    "y_pred = model.predict(X_test)                        # 4. predict on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the accuracy_score utility to see the fraction of predicted labels that match their true value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75418994413407825"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus our model achieve a test accuracy of  75% ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST¶\n",
    "\n",
    "A random forest is a 'meta estimator'. It will fit a number of decision trees (we'll have to tell it how many) on various sub-samples of the dataset. Then it will use averaging to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79329608938547491"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rForest = RandomForestClassifier()\n",
    "rForest.fit(X_train, y_train) \n",
    "y_pred = rForest.predict(X_test)  \n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus our model achieve a test accuracy of  79% ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Well that was easy! But how can we find out how well it worked?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define cross-validation\n",
    "\n",
    "In cross-validation, the data is instead split repeatedly and multiple models are trained.  The most commonly used version of cross-validation is **k-fold cross-validation**, where k is a user-specified number, usually 5 or 10. We will use 10-fold cross validation to estimate accuracy. This will split our dataset into 10 parts, train on 9 and test on 1 and repeat for all combinations of train-test splits.\n",
    "\n",
    "Cross-validation is implemented in scikit-learn using the **cross_val_score func‐\n",
    "tion** from the model_selection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def classifiers(X_train,y_train,X_test,y_test):\n",
    "    knn = KNeighborsClassifier()\n",
    "    gnb = GaussianNB()\n",
    "    logistic = LogisticRegression()\n",
    "    svc = svm.SVC()\n",
    "    dTree = tree.DecisionTreeClassifier()\n",
    "    rForest = RandomForestClassifier()\n",
    "    \n",
    "    \n",
    "    names = [\"Nearest Neighbors\", \"Naive Bayes\",\"Logistic\",\"RBF SVM\",\"Decision Tree\",\"Random Forest\"]\n",
    "    model = [knn, gnb, logistic, svc, dTree, rForest]\n",
    "    \n",
    "    \n",
    "    for (i,clf) in enumerate(classifiers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
